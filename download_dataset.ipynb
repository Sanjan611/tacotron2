{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"download_dataset.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyM7Vw7CikXNSIndSqVmDZ0D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DErThchGyF5y","executionInfo":{"status":"ok","timestamp":1621319769756,"user_tz":-330,"elapsed":2169,"user":{"displayName":"Sanjan Das","photoUrl":"","userId":"07423016562538290108"}},"outputId":"d573cef0-7d3f-4c4d-cee6-c769cd5d6d90"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvaA9oTZH-Oc","executionInfo":{"status":"ok","timestamp":1621319773012,"user_tz":-330,"elapsed":1401,"user":{"displayName":"Sanjan Das","photoUrl":"","userId":"07423016562538290108"}},"outputId":"4f11b8c1-f850-45fb-abf8-068b1b37d66c"},"source":["!pwd"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tj2dWzkiCB3g","executionInfo":{"status":"ok","timestamp":1621319781927,"user_tz":-330,"elapsed":6068,"user":{"displayName":"Sanjan Das","photoUrl":"","userId":"07423016562538290108"}},"outputId":"90b4cc2a-d7c3-45c4-c0c3-a4493554101a"},"source":["%tensorflow_version 1.x\n","import tensorflow\n","print(tensorflow.__version__)\n","!pip install Unidecode\n","# !tf_upgrade_v2 --intree tacotron2/ --inplace"],"execution_count":4,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","1.15.2\n","Requirement already satisfied: Unidecode in /usr/local/lib/python3.7/dist-packages (1.2.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lbdQm4s2Z889","executionInfo":{"status":"ok","timestamp":1621318900843,"user_tz":-330,"elapsed":186888,"user":{"displayName":"Sanjan Das","photoUrl":"","userId":"07423016562538290108"}},"outputId":"5a200e4f-f4f9-4c45-ef47-6f4957745029"},"source":["!pip install -r /content/drive/MyDrive/Summer/tacotron2/requirements.txt"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting matplotlib==2.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/90/cf10bb2020d2811da811a49601f6eafcda022c6ccd296fd05aba093dee96/matplotlib-2.1.0.tar.gz (35.7MB)\n","\u001b[K     |████████████████████████████████| 35.7MB 83kB/s \n","\u001b[?25hRequirement already satisfied: tensorflow==1.15.2 in /tensorflow-1.15.2/python3.7 (from -r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (1.15.2)\n","Collecting numpy==1.13.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/2d/005e45738ab07a26e621c9c12dc97381f372e06678adf7dc3356a69b5960/numpy-1.13.3.zip (5.0MB)\n","\u001b[K     |████████████████████████████████| 5.0MB 29.9MB/s \n","\u001b[?25hCollecting inflect==0.2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/15/2d176749884cbeda0c92e0d09e1303ff53a973eb3c6bb2136803b9d962c9/inflect-0.2.5-py2.py3-none-any.whl (58kB)\n","\u001b[K     |████████████████████████████████| 61kB 6.6MB/s \n","\u001b[?25hCollecting librosa==0.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/f4/422bfbefd581f74354ef05176aa48558c548243c87e359d91512d4b65523/librosa-0.6.0.tar.gz (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 28.8MB/s \n","\u001b[?25hCollecting scipy==1.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/73/76fc6ea21818eed0de8dd38e1e9586725578864169a2b31acdeffb9131c8/scipy-1.0.0.tar.gz (15.2MB)\n","\u001b[K     |████████████████████████████████| 15.2MB 147kB/s \n","\u001b[?25hCollecting Unidecode==1.0.22\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n","\u001b[K     |████████████████████████████████| 235kB 28.9MB/s \n","\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 8)) (7.1.2)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.0->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 1)) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.0->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 1)) (2.8.1)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.0->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 1)) (2018.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.0->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 1)) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.0->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 1)) (2.4.7)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (1.32.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (1.1.2)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (1.12.1)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (3.3.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.7 (from tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (1.0.8)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (0.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (1.1.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (0.8.1)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (0.36.2)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (3.12.4)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.7 (from tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (1.15.0)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.7 (from tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (1.15.1)\n","Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 5)) (2.1.9)\n","Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 5)) (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 5)) (1.0.1)\n","Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 5)) (4.4.2)\n","Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 5)) (0.2.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (2.10.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (56.1.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (2.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (3.3.4)\n","Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.0->librosa==0.6.0->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 5)) (0.51.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (4.0.1)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy>=0.2.0->librosa==0.6.0->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 5)) (0.34.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->-r /content/drive/MyDrive/Summer/tacotron2/requirements.txt (line 2)) (3.4.1)\n","Building wheels for collected packages: matplotlib, numpy, librosa, scipy, gast\n","  Building wheel for matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for matplotlib: filename=matplotlib-2.1.0-cp37-cp37m-linux_x86_64.whl size=10229047 sha256=7c96b245be8b00d8fee0f05053455a89e6cc378bb15598c643fee86f63dc76c5\n","  Stored in directory: /root/.cache/pip/wheels/a7/99/eb/b5566219ff5a526f98e802144d551feaa6b9340b3569bf86df\n","  Building wheel for numpy (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for numpy\u001b[0m\n","\u001b[?25h  Running setup.py clean for numpy\n","\u001b[31m  ERROR: Failed cleaning build dir for numpy\u001b[0m\n","  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for librosa: filename=librosa-0.6.0-cp37-none-any.whl size=1553496 sha256=ea77fba4305330482cc05ebe1a7fd1e065502e5ce464211455bc8d0a21c915f3\n","  Stored in directory: /root/.cache/pip/wheels/0d/19/fa/71097e2207df1cc613749f15b2f0b1972c167b36d6afc09d15\n","  Building wheel for scipy (setup.py) ... \u001b[?25lerror\n","\u001b[31m  ERROR: Failed building wheel for scipy\u001b[0m\n","\u001b[?25h  Running setup.py clean for scipy\n","\u001b[31m  ERROR: Failed cleaning build dir for scipy\u001b[0m\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=9eba259e9958781476d85b5ac8a47ff166b282fdbba5bba6f4ad8366e377fb37\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built matplotlib librosa gast\n","Failed to build numpy scipy\n","\u001b[31mERROR: lucid 0.3.10 requires umap-learn, which is not installed.\u001b[0m\n","\u001b[31mERROR: tensorflow 1.15.2 has requirement numpy<2.0,>=1.16.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: magenta 0.3.19 has requirement librosa>=0.6.2, but you'll have librosa 0.6.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: xarray 0.18.0 has requirement numpy>=1.17, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tifffile 2021.4.8 has requirement numpy>=1.15.1, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: seaborn 0.11.1 has requirement matplotlib>=2.2, but you'll have matplotlib 2.1.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: seaborn 0.11.1 has requirement numpy>=1.15, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pymc3 3.11.2 has requirement numpy>=1.15.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pymc3 3.11.2 has requirement scipy>=1.2.0, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pyerfa 1.7.3 has requirement numpy>=1.16, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pyarrow 3.0.0 has requirement numpy>=1.16.6, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 2.1.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: pandas 1.1.5 has requirement numpy>=1.15.4, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: opencv-python 4.1.2.30 has requirement numpy>=1.14.5, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: opencv-contrib-python 4.1.2.30 has requirement numpy>=1.14.5, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: numba 0.51.2 has requirement numpy>=1.15, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 2.1.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: jaxlib 0.1.66+cuda110 has requirement numpy>=1.16, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fbprophet 0.7.1 has requirement numpy>=1.15.4, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: cvxpy 1.0.31 has requirement scipy>=1.1.0, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: astropy 4.2.1 has requirement numpy>=1.17, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: arviz 0.11.2 has requirement matplotlib>=3.0, but you'll have matplotlib 2.1.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: numpy, matplotlib, inflect, scipy, librosa, Unidecode, gast\n","  Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","    Running setup.py install for numpy ... \u001b[?25l\u001b[?25herror\n","  Rolling back uninstall of numpy\n","  Moving to /usr/bin/f2py\n","   from /tmp/pip-uninstall-y2lznzly/f2py\n","  Moving to /usr/local/bin/f2py\n","   from /tmp/pip-uninstall-eh54igu6/f2py\n","  Moving to /usr/local/bin/f2py3\n","   from /tmp/pip-uninstall-eh54igu6/f2py3\n","  Moving to /usr/local/bin/f2py3.7\n","   from /tmp/pip-uninstall-eh54igu6/f2py3.7\n","  Moving to /usr/local/lib/python3.7/dist-packages/numpy-1.19.5.dist-info/\n","   from /usr/local/lib/python3.7/dist-packages/~umpy-1.19.5.dist-info\n","  Moving to /usr/local/lib/python3.7/dist-packages/numpy.libs/\n","   from /usr/local/lib/python3.7/dist-packages/~umpy.libs\n","  Moving to /usr/local/lib/python3.7/dist-packages/numpy/\n","   from /usr/local/lib/python3.7/dist-packages/~umpy\n","\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-wd4cmy4p/numpy/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-wd4cmy4p/numpy/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-hiz1i4e7/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PcnLohjmyov4","executionInfo":{"status":"ok","timestamp":1621319074065,"user_tz":-330,"elapsed":355680,"user":{"displayName":"Sanjan Das","photoUrl":"","userId":"07423016562538290108"}},"outputId":"b6b24e02-6d16-4726-e7e3-c0fdf6ce143f"},"source":["!wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2"],"execution_count":5,"outputs":[{"output_type":"stream","text":["--2021-05-18 06:21:38--  https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n","Resolving data.keithito.com (data.keithito.com)... 174.138.79.61\n","Connecting to data.keithito.com (data.keithito.com)|174.138.79.61|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2748572632 (2.6G) [application/octet-stream]\n","Saving to: ‘LJSpeech-1.1.tar.bz2’\n","\n","LJSpeech-1.1.tar.bz 100%[===================>]   2.56G  15.8MB/s    in 2m 52s  \n","\n","2021-05-18 06:24:31 (15.3 MB/s) - ‘LJSpeech-1.1.tar.bz2’ saved [2748572632/2748572632]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YVCkxRK-74kk"},"source":["https://superuser.com/questions/1466776/how-to-decompress-a-bz2-file-in-google-collab"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JRYoHGqry-Js","executionInfo":{"status":"ok","timestamp":1621319356747,"user_tz":-330,"elapsed":636288,"user":{"displayName":"Sanjan Das","photoUrl":"","userId":"07423016562538290108"}},"outputId":"894dc2b8-353b-4e98-dd78-88fc0dbe235c"},"source":["!bunzip2 -v 'LJSpeech-1.1.tar.bz2'"],"execution_count":6,"outputs":[{"output_type":"stream","text":["  LJSpeech-1.1.tar.bz2: done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"b6Jlc1wt5so-","executionInfo":{"status":"ok","timestamp":1621319422166,"user_tz":-330,"elapsed":699158,"user":{"displayName":"Sanjan Das","photoUrl":"","userId":"07423016562538290108"}}},"source":["%%capture\n","!tar -xvf 'LJSpeech-1.1.tar'"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxeoYUsgLN6c","executionInfo":{"status":"ok","timestamp":1621319801016,"user_tz":-330,"elapsed":1245,"user":{"displayName":"Sanjan Das","photoUrl":"","userId":"07423016562538290108"}},"outputId":"f721b4e4-f34d-4f96-c0cf-90e906402860"},"source":["%cd /content"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cMgEFUm6LRDV","executionInfo":{"status":"ok","timestamp":1621319804631,"user_tz":-330,"elapsed":1242,"user":{"displayName":"Sanjan Das","photoUrl":"","userId":"07423016562538290108"}},"outputId":"81dafa28-a072-409a-d6bf-76b10fe19fb2"},"source":["!rm LJSpeech-1.1.tar"],"execution_count":6,"outputs":[{"output_type":"stream","text":["rm: cannot remove 'LJSpeech-1.1.tar': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dyulX9CS9hlG","executionInfo":{"status":"ok","timestamp":1621319808184,"user_tz":-330,"elapsed":2721,"user":{"displayName":"Sanjan Das","photoUrl":"","userId":"07423016562538290108"}}},"source":["!sed -i -- 's,DUMMY,/content/LJSpeech-1.1/wavs,g' /content/drive/MyDrive/Summer/tacotron2/filelists/*.txt"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"gf196a9HYfq7","executionInfo":{"status":"ok","timestamp":1621319428011,"user_tz":-330,"elapsed":697033,"user":{"displayName":"Sanjan Das","photoUrl":"","userId":"07423016562538290108"}}},"source":["!mkdir /content/output_dir\n","!mkdir /content/log_dir"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NJtbTABRHx2y","executionInfo":{"status":"ok","timestamp":1621319816482,"user_tz":-330,"elapsed":1159,"user":{"displayName":"Sanjan Das","photoUrl":"","userId":"07423016562538290108"}},"outputId":"68058b54-c045-4fd7-b7be-525160ee8a67"},"source":["%cd /content/drive/MyDrive/Summer/tacotron2"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Summer/tacotron2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ABU5xsItK6KN","executionInfo":{"status":"ok","timestamp":1621319821481,"user_tz":-330,"elapsed":1254,"user":{"displayName":"Sanjan Das","photoUrl":"","userId":"07423016562538290108"}},"outputId":"ed7bb595-e3d0-4cf9-8cb5-ae03d4ccf18b"},"source":["!pwd"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Summer/tacotron2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1fgv6vhfZjB3","outputId":"898d4bc9-6865-437b-f15d-5a6d96fbae80"},"source":["!python train.py --output_directory=/content/output_dir --log_directory=/content/log_dir"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","FP16 Run: False\n","Dynamic Loss Scaling: True\n","Distributed Run: False\n","cuDNN Enabled: True\n","cuDNN Benchmark: False\n","Epoch: 0\n","Train loss 0 53.236374 Grad Norm 14.342931 9.30s/it\n","Validation loss 0: 36.160817  \n","Saving model and optimizer state at iteration 0 to /content/output_dir/checkpoint_0\n","Train loss 1 35.103897 Grad Norm 24.797428 8.35s/it\n","Train loss 2 16.805964 Grad Norm 17.453758 8.39s/it\n","Train loss 3 8.033684 Grad Norm 11.643000 8.28s/it\n","Train loss 4 5.647957 Grad Norm 6.196806 8.13s/it\n","Train loss 5 5.574353 Grad Norm 7.099068 8.13s/it\n","Train loss 6 5.810761 Grad Norm 7.084461 8.49s/it\n","Train loss 7 5.356900 Grad Norm 6.447299 8.70s/it\n","Train loss 8 5.451356 Grad Norm 5.416644 8.55s/it\n","Train loss 9 5.329153 Grad Norm 4.398016 8.03s/it\n","Train loss 10 4.801016 Grad Norm 2.460231 8.44s/it\n","Train loss 11 4.590910 Grad Norm 2.019503 7.99s/it\n","Train loss 12 5.002547 Grad Norm 2.745217 8.18s/it\n","Train loss 13 4.842455 Grad Norm 3.014055 8.54s/it\n","Train loss 14 4.570536 Grad Norm 4.209762 8.70s/it\n","Train loss 15 4.718475 Grad Norm 3.153138 8.67s/it\n","Train loss 16 5.049237 Grad Norm 3.088858 8.20s/it\n","Train loss 17 4.321861 Grad Norm 2.552199 8.36s/it\n","Train loss 18 3.995049 Grad Norm 2.331812 8.24s/it\n","Train loss 19 4.142404 Grad Norm 2.240274 8.38s/it\n","Train loss 20 3.778514 Grad Norm 2.072503 8.41s/it\n","Train loss 21 4.797626 Grad Norm 2.547164 8.28s/it\n","Train loss 22 3.679106 Grad Norm 1.689523 8.21s/it\n","Train loss 23 4.319515 Grad Norm 1.911616 8.50s/it\n","Train loss 24 4.085410 Grad Norm 1.810678 8.22s/it\n","Train loss 25 3.753216 Grad Norm 2.096776 8.41s/it\n","Train loss 26 3.850508 Grad Norm 1.836625 8.48s/it\n","Train loss 27 4.097418 Grad Norm 2.001406 7.98s/it\n","Train loss 28 3.651386 Grad Norm 2.225342 8.38s/it\n","Train loss 29 3.764693 Grad Norm 2.592451 8.79s/it\n","Train loss 30 3.946381 Grad Norm 3.321285 8.18s/it\n","Train loss 31 3.887072 Grad Norm 5.522473 8.33s/it\n","Train loss 32 3.702940 Grad Norm 5.724768 8.25s/it\n","Train loss 33 3.298295 Grad Norm 4.061980 8.28s/it\n","Train loss 34 3.331271 Grad Norm 4.825603 8.52s/it\n","Train loss 35 3.829344 Grad Norm 6.075784 7.89s/it\n","Train loss 36 3.808271 Grad Norm 3.328036 8.36s/it\n","Train loss 37 3.505133 Grad Norm 8.475760 8.38s/it\n","Train loss 38 4.393905 Grad Norm 10.130118 8.52s/it\n","Train loss 39 3.803651 Grad Norm 6.888205 8.72s/it\n","Train loss 40 3.173806 Grad Norm 2.621912 8.45s/it\n","Train loss 41 3.604924 Grad Norm 4.424698 8.56s/it\n","Train loss 42 4.104638 Grad Norm 6.309229 8.64s/it\n","Train loss 43 3.061045 Grad Norm 1.560141 8.65s/it\n","Train loss 44 3.589447 Grad Norm 5.618617 8.65s/it\n","Train loss 45 3.945101 Grad Norm 6.762959 8.40s/it\n","Train loss 46 3.549396 Grad Norm 4.422250 8.60s/it\n","Train loss 47 3.662221 Grad Norm 4.170902 8.65s/it\n","Train loss 48 3.681797 Grad Norm 6.208585 8.56s/it\n","Train loss 49 3.685855 Grad Norm 5.743667 8.32s/it\n","Train loss 50 3.456633 Grad Norm 4.668681 8.59s/it\n","Train loss 51 3.316767 Grad Norm 6.824807 8.57s/it\n","Train loss 52 3.599300 Grad Norm 3.667948 8.38s/it\n","Train loss 53 3.509587 Grad Norm 5.954826 8.34s/it\n","Train loss 54 3.418940 Grad Norm 6.136260 8.17s/it\n","Train loss 55 3.432673 Grad Norm 3.868811 8.54s/it\n","Train loss 56 3.419731 Grad Norm 5.674497 8.30s/it\n","Train loss 57 3.458616 Grad Norm 7.558558 8.49s/it\n","Train loss 58 3.355575 Grad Norm 4.242594 8.79s/it\n","Train loss 59 3.162594 Grad Norm 3.524110 8.54s/it\n","Train loss 60 3.242557 Grad Norm 5.036208 8.62s/it\n","Train loss 61 3.017582 Grad Norm 2.051624 8.71s/it\n","Train loss 62 3.563271 Grad Norm 5.267340 8.40s/it\n","Train loss 63 3.454027 Grad Norm 7.088209 8.44s/it\n","Train loss 64 3.111058 Grad Norm 3.394954 8.55s/it\n","Train loss 65 2.736152 Grad Norm 1.821104 8.44s/it\n","Train loss 66 3.457875 Grad Norm 4.387006 8.74s/it\n","Train loss 67 3.596485 Grad Norm 2.877037 8.54s/it\n","Train loss 68 3.007574 Grad Norm 4.973565 8.31s/it\n","Train loss 69 2.875517 Grad Norm 4.388080 8.57s/it\n","Train loss 70 3.285188 Grad Norm 2.110583 8.53s/it\n","Train loss 71 3.428777 Grad Norm 4.521523 8.11s/it\n","Train loss 72 3.037205 Grad Norm 2.666898 8.49s/it\n","Train loss 73 2.972417 Grad Norm 0.953157 8.63s/it\n","Train loss 74 3.006988 Grad Norm 2.811025 8.12s/it\n","Train loss 75 3.134088 Grad Norm 2.996850 8.49s/it\n","Train loss 76 3.111035 Grad Norm 3.985069 8.21s/it\n","Train loss 77 2.881981 Grad Norm 3.741672 8.59s/it\n","Train loss 78 3.307540 Grad Norm 3.456077 8.01s/it\n","Train loss 79 2.946790 Grad Norm 1.550416 8.37s/it\n","Train loss 80 2.802953 Grad Norm 4.552245 8.60s/it\n","Train loss 81 3.134607 Grad Norm 2.326309 8.30s/it\n","Train loss 82 2.821215 Grad Norm 2.874200 7.92s/it\n","Train loss 83 3.722336 Grad Norm 3.231837 8.22s/it\n","Train loss 84 3.252142 Grad Norm 4.377896 8.33s/it\n","Train loss 85 3.095476 Grad Norm 3.730652 7.76s/it\n","Train loss 86 3.107878 Grad Norm 2.348179 8.57s/it\n","Train loss 87 2.958464 Grad Norm 2.022276 8.80s/it\n","Train loss 88 3.027156 Grad Norm 3.279570 8.44s/it\n","Train loss 89 2.891372 Grad Norm 2.237322 8.51s/it\n","Train loss 90 2.614780 Grad Norm 2.133248 8.55s/it\n","Train loss 91 2.769326 Grad Norm 2.841001 8.50s/it\n","Train loss 92 3.100310 Grad Norm 2.699539 8.50s/it\n","Train loss 93 2.878890 Grad Norm 7.780176 8.49s/it\n","Train loss 94 3.412366 Grad Norm 9.148219 8.44s/it\n","Train loss 95 3.235796 Grad Norm 3.231715 8.72s/it\n","Train loss 96 2.895796 Grad Norm 5.600116 8.31s/it\n","Train loss 97 2.730566 Grad Norm 3.381109 8.24s/it\n","Train loss 98 3.194900 Grad Norm 4.080279 8.74s/it\n","Train loss 99 2.884033 Grad Norm 7.137873 8.30s/it\n","Train loss 100 3.155648 Grad Norm 8.244554 8.14s/it\n","Train loss 101 2.803222 Grad Norm 4.954480 8.19s/it\n","Train loss 102 2.641636 Grad Norm 3.118546 8.51s/it\n","Train loss 103 3.030223 Grad Norm 6.508883 8.52s/it\n","Train loss 104 2.702037 Grad Norm 3.384900 8.57s/it\n","Train loss 105 2.990210 Grad Norm 2.668351 8.55s/it\n","Train loss 106 2.763846 Grad Norm 5.205671 8.62s/it\n","Train loss 107 2.833253 Grad Norm 4.103960 8.41s/it\n","Train loss 108 2.638820 Grad Norm 2.208424 8.68s/it\n","Train loss 109 2.890642 Grad Norm 4.290588 8.71s/it\n","Train loss 110 2.497785 Grad Norm 2.035320 8.21s/it\n","Train loss 111 2.731557 Grad Norm 4.694983 8.64s/it\n","Train loss 112 2.563888 Grad Norm 6.674903 8.29s/it\n","Train loss 113 2.710385 Grad Norm 3.081809 8.54s/it\n","Train loss 114 2.794644 Grad Norm 6.100419 8.23s/it\n","Train loss 115 2.485374 Grad Norm 6.561713 8.61s/it\n","Train loss 116 2.536235 Grad Norm 4.351879 8.71s/it\n","Train loss 117 2.487686 Grad Norm 4.189700 8.42s/it\n","Train loss 118 2.894092 Grad Norm 5.348341 8.40s/it\n","Train loss 119 2.389476 Grad Norm 2.077572 8.40s/it\n","Train loss 120 2.366290 Grad Norm 4.046170 8.36s/it\n","Train loss 121 2.615088 Grad Norm 5.736495 8.43s/it\n","Train loss 122 2.313020 Grad Norm 2.708929 8.44s/it\n","Train loss 123 2.134423 Grad Norm 3.980725 8.45s/it\n","Train loss 124 2.305276 Grad Norm 3.761414 8.34s/it\n","Train loss 125 2.073984 Grad Norm 2.390097 8.60s/it\n","Train loss 126 2.481275 Grad Norm 3.774651 8.17s/it\n","Train loss 127 2.075830 Grad Norm 4.977625 8.43s/it\n","Train loss 128 2.358576 Grad Norm 3.627773 8.52s/it\n","Train loss 129 2.546341 Grad Norm 2.568194 8.41s/it\n","Train loss 130 2.321864 Grad Norm 4.138498 8.59s/it\n","Train loss 131 2.744295 Grad Norm 3.352198 8.49s/it\n","Train loss 132 2.309177 Grad Norm 2.897827 8.43s/it\n","Train loss 133 2.019087 Grad Norm 3.735337 7.99s/it\n","Train loss 134 2.181803 Grad Norm 5.297883 8.55s/it\n","Train loss 135 1.971951 Grad Norm 1.697206 8.53s/it\n","Train loss 136 2.097782 Grad Norm 2.849840 8.33s/it\n","Train loss 137 2.256156 Grad Norm 2.640235 8.47s/it\n","Train loss 138 2.358572 Grad Norm 3.332696 8.43s/it\n","Train loss 139 2.243604 Grad Norm 4.099069 8.62s/it\n","Train loss 140 2.203988 Grad Norm 3.736928 8.53s/it\n","Train loss 141 2.168816 Grad Norm 3.162624 8.06s/it\n","Train loss 142 2.175817 Grad Norm 2.721081 8.48s/it\n","Train loss 143 2.083132 Grad Norm 3.623888 8.49s/it\n","Train loss 144 2.338780 Grad Norm 4.106921 8.37s/it\n","Train loss 145 2.101633 Grad Norm 2.536637 8.59s/it\n","Train loss 146 1.978641 Grad Norm 3.850961 8.12s/it\n","Train loss 147 1.939998 Grad Norm 2.834683 8.62s/it\n","Train loss 148 1.938923 Grad Norm 3.764916 8.35s/it\n","Train loss 149 2.280744 Grad Norm 5.087337 8.39s/it\n","Train loss 150 2.064027 Grad Norm 4.322093 8.08s/it\n","Train loss 151 2.194810 Grad Norm 6.167490 8.66s/it\n","Train loss 152 2.013998 Grad Norm 2.698525 7.93s/it\n","Train loss 153 2.126290 Grad Norm 7.222784 8.62s/it\n","Train loss 154 2.563341 Grad Norm 11.205117 8.47s/it\n","Train loss 155 2.112288 Grad Norm 5.578077 8.17s/it\n","Train loss 156 2.032277 Grad Norm 3.955570 8.57s/it\n","Train loss 157 2.147335 Grad Norm 5.362520 8.67s/it\n","Train loss 158 2.120520 Grad Norm 1.351507 8.55s/it\n","Train loss 159 1.993470 Grad Norm 1.712894 8.53s/it\n","Train loss 160 2.077248 Grad Norm 2.097240 8.50s/it\n","Train loss 161 2.062352 Grad Norm 2.238043 8.29s/it\n","Train loss 162 1.951500 Grad Norm 2.991450 8.32s/it\n","Train loss 163 2.226385 Grad Norm 5.162166 8.61s/it\n","Train loss 164 1.899576 Grad Norm 2.568520 8.30s/it\n","Train loss 165 1.906090 Grad Norm 2.917989 8.57s/it\n","Train loss 166 1.831343 Grad Norm 3.082670 8.55s/it\n","Train loss 167 1.900999 Grad Norm 4.210077 8.44s/it\n","Train loss 168 1.975323 Grad Norm 4.287532 8.46s/it\n","Train loss 169 2.022760 Grad Norm 2.527160 8.69s/it\n","Train loss 170 2.034077 Grad Norm 2.480136 8.51s/it\n","Train loss 171 1.941346 Grad Norm 1.994019 8.36s/it\n","Train loss 172 1.886371 Grad Norm 1.223014 8.36s/it\n","Train loss 173 1.931553 Grad Norm 3.805594 8.45s/it\n","Train loss 174 1.966266 Grad Norm 3.447870 8.43s/it\n","Train loss 175 2.123620 Grad Norm 4.986973 8.36s/it\n","Train loss 176 2.174998 Grad Norm 6.693180 8.58s/it\n","Train loss 177 1.975100 Grad Norm 3.100236 8.13s/it\n","Train loss 178 1.928133 Grad Norm 4.661538 7.77s/it\n","Train loss 179 2.062681 Grad Norm 6.563219 8.29s/it\n","Train loss 180 2.215228 Grad Norm 7.251880 8.30s/it\n","Train loss 181 2.005644 Grad Norm 2.177722 8.70s/it\n","Train loss 182 1.981830 Grad Norm 3.426273 8.43s/it\n","Train loss 183 2.005575 Grad Norm 2.096357 8.34s/it\n","Train loss 184 1.765134 Grad Norm 1.185188 8.50s/it\n","Train loss 185 2.013642 Grad Norm 5.888630 8.69s/it\n","Train loss 186 1.857567 Grad Norm 4.810439 8.26s/it\n","Train loss 187 1.953621 Grad Norm 1.698120 8.39s/it\n","Train loss 188 2.072018 Grad Norm 6.124991 8.50s/it\n","Train loss 189 2.132326 Grad Norm 6.507099 8.45s/it\n","Train loss 190 1.795653 Grad Norm 2.614337 8.55s/it\n","Train loss 191 1.861211 Grad Norm 3.980628 8.48s/it\n","Train loss 192 1.925027 Grad Norm 6.767527 8.46s/it\n","Train loss 193 2.360133 Grad Norm 9.170626 8.46s/it\n","Train loss 194 1.808806 Grad Norm 1.865253 8.49s/it\n","Train loss 195 1.975564 Grad Norm 2.478461 8.47s/it\n","Train loss 196 1.747347 Grad Norm 4.830751 8.07s/it\n","Train loss 197 1.738631 Grad Norm 1.926685 8.31s/it\n","Train loss 198 2.387577 Grad Norm 8.422984 8.00s/it\n","Train loss 199 2.085075 Grad Norm 6.809588 8.38s/it\n","Train loss 200 2.000742 Grad Norm 3.647183 8.42s/it\n","Train loss 201 1.923758 Grad Norm 5.128470 8.25s/it\n","Train loss 202 1.960449 Grad Norm 4.172151 8.31s/it\n","Train loss 203 1.892250 Grad Norm 3.844349 8.47s/it\n","Train loss 204 1.994818 Grad Norm 4.544087 8.24s/it\n","Train loss 205 1.776929 Grad Norm 1.547827 8.50s/it\n","Train loss 206 1.957277 Grad Norm 2.766383 8.50s/it\n","Train loss 207 1.860749 Grad Norm 2.720294 8.34s/it\n","Train loss 208 1.912864 Grad Norm 2.926296 8.55s/it\n","Train loss 209 1.954735 Grad Norm 2.051301 8.00s/it\n","Train loss 210 1.765376 Grad Norm 1.069879 8.28s/it\n","Train loss 211 1.703803 Grad Norm 2.444725 8.16s/it\n","Train loss 212 1.714102 Grad Norm 1.129968 8.65s/it\n","Train loss 213 1.888396 Grad Norm 2.152977 8.40s/it\n","Train loss 214 2.331541 Grad Norm 2.706831 8.45s/it\n","Train loss 215 2.046082 Grad Norm 4.373134 8.11s/it\n","Train loss 216 1.967886 Grad Norm 1.920381 8.49s/it\n","Train loss 217 1.960080 Grad Norm 1.599545 8.05s/it\n","Train loss 218 1.714616 Grad Norm 3.215643 8.45s/it\n","Train loss 219 1.802059 Grad Norm 2.526103 8.31s/it\n","Train loss 220 1.782288 Grad Norm 1.758583 8.46s/it\n","Train loss 221 1.797197 Grad Norm 2.624231 8.35s/it\n","Train loss 222 1.902658 Grad Norm 2.597304 8.55s/it\n","Train loss 223 1.732568 Grad Norm 1.388689 8.27s/it\n","Train loss 224 1.841918 Grad Norm 2.482665 8.65s/it\n","Train loss 225 1.709845 Grad Norm 2.007667 8.03s/it\n","Train loss 226 1.791352 Grad Norm 1.232009 8.60s/it\n","Train loss 227 2.038380 Grad Norm 2.732513 8.37s/it\n","Train loss 228 1.824039 Grad Norm 5.052185 8.67s/it\n","Train loss 229 2.207037 Grad Norm 2.724084 8.56s/it\n","Train loss 230 1.718962 Grad Norm 1.034938 8.44s/it\n","Train loss 231 1.955249 Grad Norm 2.332734 8.42s/it\n","Train loss 232 1.910277 Grad Norm 2.607246 8.51s/it\n","Train loss 233 1.724862 Grad Norm 1.969185 8.60s/it\n","Train loss 234 1.746110 Grad Norm 1.264357 8.57s/it\n","Train loss 235 1.749121 Grad Norm 1.795084 8.62s/it\n","Train loss 236 1.947012 Grad Norm 3.828414 8.48s/it\n","Train loss 237 1.906381 Grad Norm 2.739290 8.25s/it\n","Train loss 238 1.973864 Grad Norm 2.014323 8.20s/it\n","Train loss 239 1.752895 Grad Norm 1.849684 8.41s/it\n","Train loss 240 1.692222 Grad Norm 1.323084 8.19s/it\n","Train loss 241 1.898720 Grad Norm 6.858861 8.20s/it\n","Train loss 242 2.265508 Grad Norm 8.949428 8.46s/it\n","Train loss 243 1.950390 Grad Norm 3.172347 8.57s/it\n","Train loss 244 1.840382 Grad Norm 7.043478 8.69s/it\n","Train loss 245 1.963838 Grad Norm 7.369771 8.64s/it\n","Train loss 246 1.718514 Grad Norm 5.622824 8.56s/it\n","Train loss 247 1.962763 Grad Norm 5.398705 8.06s/it\n","Train loss 248 1.892825 Grad Norm 5.956458 8.60s/it\n","Train loss 249 1.943455 Grad Norm 5.107059 8.57s/it\n","Train loss 250 1.884258 Grad Norm 5.236901 8.75s/it\n","Train loss 251 1.987106 Grad Norm 6.461588 8.54s/it\n","Train loss 252 1.873837 Grad Norm 2.615859 8.60s/it\n","Train loss 253 1.916278 Grad Norm 4.320509 8.45s/it\n","Train loss 254 1.762606 Grad Norm 3.218833 8.50s/it\n","Train loss 255 1.821606 Grad Norm 2.520238 8.50s/it\n","Train loss 256 1.860467 Grad Norm 6.215678 8.40s/it\n","Train loss 257 2.145458 Grad Norm 8.324199 8.63s/it\n","Train loss 258 1.883263 Grad Norm 5.905620 8.08s/it\n","Train loss 259 1.713109 Grad Norm 1.563456 8.20s/it\n","Train loss 260 1.979320 Grad Norm 6.254827 8.35s/it\n","Train loss 261 1.814030 Grad Norm 4.690797 8.57s/it\n","Train loss 262 1.830798 Grad Norm 2.381739 8.45s/it\n","Train loss 263 1.967545 Grad Norm 7.493820 8.39s/it\n","Train loss 264 1.992433 Grad Norm 8.069593 8.50s/it\n","Train loss 265 1.992105 Grad Norm 7.116671 7.97s/it\n","Train loss 266 1.869523 Grad Norm 2.828591 8.46s/it\n","Train loss 267 1.827057 Grad Norm 6.701238 8.21s/it\n","Train loss 268 2.096307 Grad Norm 9.161530 8.33s/it\n","Train loss 269 2.141358 Grad Norm 8.326949 8.19s/it\n","Train loss 270 1.907688 Grad Norm 4.306571 8.43s/it\n","Train loss 271 1.943886 Grad Norm 5.836744 8.07s/it\n","Train loss 272 1.925171 Grad Norm 7.804894 8.33s/it\n","Train loss 273 1.974271 Grad Norm 6.419912 8.50s/it\n","Train loss 274 1.826043 Grad Norm 4.361473 8.48s/it\n","Train loss 275 1.813027 Grad Norm 5.133497 8.38s/it\n","Train loss 276 1.618322 Grad Norm 3.155130 8.44s/it\n","Train loss 277 1.928471 Grad Norm 4.053410 8.53s/it\n","Train loss 278 2.057319 Grad Norm 5.867972 8.59s/it\n","Train loss 279 2.119661 Grad Norm 5.925095 8.55s/it\n","Train loss 280 1.909809 Grad Norm 5.651901 8.54s/it\n","Train loss 281 1.607743 Grad Norm 2.402818 8.70s/it\n","Train loss 282 2.269310 Grad Norm 9.220267 7.58s/it\n","Train loss 283 2.276951 Grad Norm 10.475794 8.08s/it\n","Train loss 284 1.900822 Grad Norm 7.632435 8.51s/it\n","Train loss 285 2.087769 Grad Norm 7.075348 8.40s/it\n","Train loss 286 1.705970 Grad Norm 1.597106 8.13s/it\n","Train loss 287 2.013118 Grad Norm 4.297169 8.33s/it\n","Train loss 288 2.020041 Grad Norm 5.214545 8.00s/it\n","Train loss 289 1.930502 Grad Norm 3.329612 8.53s/it\n","Train loss 290 1.655988 Grad Norm 1.770524 8.50s/it\n","Train loss 291 1.813939 Grad Norm 5.116687 8.44s/it\n","Train loss 292 2.111916 Grad Norm 7.211277 8.66s/it\n","Train loss 293 2.136235 Grad Norm 6.220601 8.36s/it\n","Train loss 294 1.914155 Grad Norm 5.016047 8.48s/it\n","Train loss 295 1.728803 Grad Norm 4.376439 8.61s/it\n","Train loss 296 1.790689 Grad Norm 2.901752 8.62s/it\n","Train loss 297 1.703205 Grad Norm 2.806964 8.56s/it\n","Train loss 298 1.569607 Grad Norm 2.749110 8.25s/it\n","Train loss 299 1.964334 Grad Norm 3.801125 8.29s/it\n","Train loss 300 1.920447 Grad Norm 2.633884 8.30s/it\n","Train loss 301 2.077352 Grad Norm 5.621027 8.45s/it\n","Train loss 302 1.827408 Grad Norm 5.365731 8.46s/it\n","Train loss 303 1.830312 Grad Norm 2.223093 7.88s/it\n","Train loss 304 1.920349 Grad Norm 3.055951 8.27s/it\n","Train loss 305 1.667331 Grad Norm 1.931021 8.58s/it\n","Train loss 306 1.706156 Grad Norm 1.265822 8.27s/it\n","Train loss 307 1.753960 Grad Norm 2.882061 8.58s/it\n","Train loss 308 1.745916 Grad Norm 1.468020 7.63s/it\n","Train loss 309 1.900387 Grad Norm 4.421866 8.54s/it\n","Train loss 310 1.894752 Grad Norm 4.757686 7.98s/it\n","Train loss 311 1.720317 Grad Norm 1.743188 8.47s/it\n","Train loss 312 1.652829 Grad Norm 3.391921 8.47s/it\n","Train loss 313 1.729436 Grad Norm 5.249599 8.18s/it\n","Train loss 314 1.664685 Grad Norm 2.640327 8.45s/it\n","Train loss 315 1.727815 Grad Norm 3.884382 8.56s/it\n","Train loss 316 1.803552 Grad Norm 3.854542 8.40s/it\n","Train loss 317 1.639288 Grad Norm 1.874058 8.36s/it\n","Train loss 318 1.697421 Grad Norm 3.504502 8.27s/it\n","Train loss 319 1.696691 Grad Norm 4.587287 8.51s/it\n","Train loss 320 1.598753 Grad Norm 1.226964 8.53s/it\n","Train loss 321 1.802037 Grad Norm 4.166624 8.56s/it\n","Train loss 322 1.920616 Grad Norm 6.073757 8.54s/it\n","Train loss 323 1.641320 Grad Norm 3.678934 8.61s/it\n","Train loss 324 1.878729 Grad Norm 6.335180 8.17s/it\n","Train loss 325 2.036878 Grad Norm 8.743246 8.70s/it\n","Train loss 326 1.945072 Grad Norm 6.715385 7.43s/it\n","Train loss 327 1.755505 Grad Norm 1.649275 8.31s/it\n","Train loss 328 1.769740 Grad Norm 5.592719 8.47s/it\n","Train loss 329 1.902416 Grad Norm 7.143613 8.43s/it\n","Train loss 330 1.761215 Grad Norm 4.289910 8.51s/it\n","Train loss 331 1.881538 Grad Norm 4.435578 8.39s/it\n","Train loss 332 1.637110 Grad Norm 3.243508 8.46s/it\n","Train loss 333 1.610669 Grad Norm 2.338408 8.47s/it\n","Train loss 334 1.632982 Grad Norm 2.764264 8.35s/it\n","Train loss 335 1.735374 Grad Norm 1.867216 8.61s/it\n","Train loss 336 1.719471 Grad Norm 1.943338 8.48s/it\n","Train loss 337 1.830603 Grad Norm 1.865672 8.08s/it\n","Train loss 338 1.721709 Grad Norm 3.929355 8.50s/it\n","Train loss 339 1.656518 Grad Norm 3.931812 8.21s/it\n","Train loss 340 1.612454 Grad Norm 1.992110 8.49s/it\n","Train loss 341 1.766450 Grad Norm 4.417374 8.13s/it\n","Train loss 342 1.633546 Grad Norm 3.971740 8.57s/it\n","Train loss 343 1.573490 Grad Norm 1.589234 8.61s/it\n","Train loss 344 1.674778 Grad Norm 4.195244 8.59s/it\n","Train loss 345 1.782781 Grad Norm 3.243113 8.55s/it\n","Train loss 346 1.515833 Grad Norm 1.642098 8.33s/it\n","Train loss 347 1.604765 Grad Norm 4.609849 8.60s/it\n","Train loss 348 1.798234 Grad Norm 4.969682 8.28s/it\n","Train loss 349 1.582251 Grad Norm 2.332164 8.42s/it\n","Train loss 350 1.853696 Grad Norm 3.252059 8.64s/it\n","Train loss 351 1.744381 Grad Norm 2.880464 8.25s/it\n","Train loss 352 1.757306 Grad Norm 2.500498 8.20s/it\n","Train loss 353 1.606485 Grad Norm 3.214818 8.27s/it\n","Train loss 354 1.669003 Grad Norm 1.174847 8.40s/it\n","Train loss 355 1.637650 Grad Norm 1.434838 8.22s/it\n","Train loss 356 1.597721 Grad Norm 2.658946 8.41s/it\n","Train loss 357 1.581325 Grad Norm 1.780123 8.38s/it\n","Train loss 358 1.782691 Grad Norm 4.875047 8.54s/it\n","Train loss 359 1.671948 Grad Norm 2.757279 8.68s/it\n","Train loss 360 1.681096 Grad Norm 4.402818 8.48s/it\n","Train loss 361 1.739706 Grad Norm 6.118394 8.25s/it\n","Train loss 362 1.601777 Grad Norm 3.174643 8.37s/it\n","Train loss 363 1.543257 Grad Norm 3.601513 8.57s/it\n","Train loss 364 1.577872 Grad Norm 5.647332 8.23s/it\n","Train loss 365 1.789679 Grad Norm 4.761005 8.54s/it\n","Train loss 366 1.630543 Grad Norm 2.813399 8.35s/it\n","Train loss 367 1.581244 Grad Norm 4.598306 7.78s/it\n","Train loss 368 1.537638 Grad Norm 3.094529 8.66s/it\n","Train loss 369 1.546719 Grad Norm 3.821718 8.06s/it\n","Train loss 370 1.579233 Grad Norm 4.509514 8.56s/it\n","Train loss 371 1.708929 Grad Norm 3.328876 8.56s/it\n","Train loss 372 1.619194 Grad Norm 4.057697 8.61s/it\n","Train loss 373 1.715796 Grad Norm 6.038604 8.54s/it\n","Train loss 374 1.580441 Grad Norm 4.096082 8.41s/it\n","Train loss 375 1.657584 Grad Norm 2.823256 8.11s/it\n","Train loss 376 1.465822 Grad Norm 2.680856 7.96s/it\n","Train loss 377 1.487014 Grad Norm 1.165048 8.32s/it\n","Train loss 378 1.652271 Grad Norm 5.373085 8.35s/it\n","Train loss 379 1.662014 Grad Norm 6.070853 8.21s/it\n","Train loss 380 1.508209 Grad Norm 2.812225 8.54s/it\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P8mTosRJK-Lm"},"source":[""],"execution_count":null,"outputs":[]}]}